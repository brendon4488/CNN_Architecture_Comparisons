{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e4fbc0f",
   "metadata": {},
   "source": [
    "### Property Price Prediction Model: Comprehensive Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f8fbd",
   "metadata": {},
   "source": [
    "**Primary Goal**  \n",
    "Conduct a targeted comparative analysis of 12 strategically selected CNN architectures to identify the optimal feature extraction backbone for property price prediction, representing key evolutionary milestones in computer vision.\n",
    "\n",
    "Primarily the models are 21 but because of reasons named below only 12 were carefully selected:\n",
    "1. Architectures that introduced fundamental design breakthoroughs\n",
    "2. Balanced considerations of practical performance vs. theoratical performance\n",
    "3. the selected models represents a distinct architetural philosopy\n",
    "\n",
    "*Core Research Question*  \n",
    "Which convolutional neural network architecture provides the most discriminative visual features when combined with tabular property data for accurate price prediction?\n",
    "\n",
    "DIVE INTO CODE AND SEE HOW THE QUESTION IS ANSWERED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e192ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107a017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scraped_page</th>\n",
       "      <th>title</th>\n",
       "      <th>detail_url</th>\n",
       "      <th>currency</th>\n",
       "      <th>price</th>\n",
       "      <th>building_area</th>\n",
       "      <th>building_unit</th>\n",
       "      <th>land_area</th>\n",
       "      <th>land_unit</th>\n",
       "      <th>property_type</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>location</th>\n",
       "      <th>image_count</th>\n",
       "      <th>image_filenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Developers Dream</td>\n",
       "      <td>https://www.property.co.zw/for-sale/houses-bls...</td>\n",
       "      <td>USD</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>1352.0</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>3 Bedroom House</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Belvedere</td>\n",
       "      <td>1</td>\n",
       "      <td>df03d95d0b_0.webp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2 Bedroom Flat In Prime Avondale Location</td>\n",
       "      <td>https://www.property.co.zw/for-sale/flats-apar...</td>\n",
       "      <td>USD</td>\n",
       "      <td>95000.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>2 Bedroom Flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Avondale</td>\n",
       "      <td>1</td>\n",
       "      <td>41c6aa94bc_0.webp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Charming 3-Bedroom Family Home in Mabvazuva, R...</td>\n",
       "      <td>https://www.property.co.zw/for-sale/houses-p19...</td>\n",
       "      <td>USD</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>3410.0</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>410.0</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>3 Bedroom House</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>483115d1c1_0.webp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The Strand office land in Borrowdale.</td>\n",
       "      <td>https://www.property.co.zw/for-sale/commercial...</td>\n",
       "      <td>USD</td>\n",
       "      <td>875000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Borrowdale</td>\n",
       "      <td>1</td>\n",
       "      <td>ac7160491a_0.webp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Stands for Sale</td>\n",
       "      <td>https://www.property.co.zw/for-sale/residentia...</td>\n",
       "      <td>USD</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>442.0</td>\n",
       "      <td>mÂ²</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harare</td>\n",
       "      <td>1</td>\n",
       "      <td>1d7f22fc05_0.webp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scraped_page                                              title  \\\n",
       "0             1                                   Developers Dream   \n",
       "1             1          2 Bedroom Flat In Prime Avondale Location   \n",
       "2             1  Charming 3-Bedroom Family Home in Mabvazuva, R...   \n",
       "3             1              The Strand office land in Borrowdale.   \n",
       "4             1                                    Stands for Sale   \n",
       "\n",
       "                                          detail_url currency     price  \\\n",
       "0  https://www.property.co.zw/for-sale/houses-bls...      USD  190000.0   \n",
       "1  https://www.property.co.zw/for-sale/flats-apar...      USD   95000.0   \n",
       "2  https://www.property.co.zw/for-sale/houses-p19...      USD  105000.0   \n",
       "3  https://www.property.co.zw/for-sale/commercial...      USD  875000.0   \n",
       "4  https://www.property.co.zw/for-sale/residentia...      USD   60000.0   \n",
       "\n",
       "   building_area building_unit  land_area land_unit    property_type  \\\n",
       "0          180.0            mÂ²     1352.0        mÂ²  3 Bedroom House   \n",
       "1          120.0            mÂ²        NaN        mÂ²   2 Bedroom Flat   \n",
       "2         3410.0            mÂ²      410.0        mÂ²  3 Bedroom House   \n",
       "3            NaN            mÂ²     8000.0        mÂ²              NaN   \n",
       "4          442.0            mÂ²      442.0        mÂ²              NaN   \n",
       "\n",
       "   bedrooms  bathrooms    location  image_count    image_filenames  \n",
       "0       3.0        1.0   Belvedere            1  df03d95d0b_0.webp  \n",
       "1       2.0        1.0    Avondale            1  41c6aa94bc_0.webp  \n",
       "2       3.0        NaN         NaN            1  483115d1c1_0.webp  \n",
       "3       NaN        NaN  Borrowdale            1  ac7160491a_0.webp  \n",
       "4       NaN        NaN      Harare            1  1d7f22fc05_0.webp  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Brendon\\Desktop\\Deep_Learning\\ASSIGNMENTS\\final_zimbabwe_property_listings_complete.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d7eee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "888b886b",
   "metadata": {},
   "source": [
    "**DATA PREPROCESSING AND FEATURE EXTRACTION**\n",
    "\n",
    "This preprocessing pipeline systematically transforms raw property listing data into a structured, machine-learning-ready format through three key phases:\n",
    "1. Data Validation and Cleaning, where invalid prices are filtered out and all currency values are standardized to USD to ensure consistency. Missing values in critical numerical features like bedrooms, bathrooms, and area measurements are intelligently handled using median imputation, which preserves the data distribution while handling gaps effectively.  \n",
    "\n",
    "2. The Feature Engineering,  demonstrates domain expertise in real estate analytics by creating meaningful derived features. Boolean flags (has_building, has_land) elegantly handle properties with missing area data, while ratio features (area_ratio, bed_bath_ratio) capture important property characteristics that raw measurements alone cannot express. The price_per_sqm metric provides a normalized pricing benchmark that enables fair comparisons across different property types and sizes.\n",
    "\n",
    "3. Text Processing, extracts property types from listing titles when explicit classifications are missing, ensuring no valuable information is lost. Categorical variables like location and property type are encoded numerically, making them compatible with machine learning algorithms while preserving their informational value.\n",
    "\n",
    "Finally, the standardization step ensures all features are on comparable scales, which is crucial for models sensitive to feature magnitudes. The output is a clean, well-structured dataset ready for both traditional machine learning and advanced neural network approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187a4b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features shape: (1610, 12)\n",
      "Target shape: (1610,)\n",
      "Features: ['building_area', 'land_area', 'bedrooms', 'bathrooms', 'has_building', 'has_land', 'area_ratio', 'bed_bath_ratio', 'price_per_sqm', 'image_count', 'property_type_encoded', 'location_encoded']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def preprocess_property_data(df):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    \n",
    "    df_clean = df_clean[df_clean['price'] > 0]  # Remove invalid prices\n",
    "    df_clean['currency'] = df_clean['currency'].fillna('USD')  # All prices in USD\n",
    "    \n",
    "    # Loop to impute with median\n",
    "    for col in ['bedrooms', 'bathrooms', 'building_area', 'land_area']:\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    \n",
    "    # Feature engineering---\n",
    "    df_clean = df_clean.assign(\n",
    "        has_building=(df_clean['building_area'] > 0).astype(int),\n",
    "        has_land=(df_clean['land_area'] > 0).astype(int),\n",
    "        area_ratio=df_clean['building_area'] / (df_clean['land_area'] + 1),\n",
    "        bed_bath_ratio=df_clean['bedrooms'] / (df_clean['bathrooms'] + 1),\n",
    "        price_per_sqm=df_clean['price'] / (df_clean['building_area'] + df_clean['land_area'] + 1)\n",
    "    )\n",
    "    \n",
    "    # Property type standardization\n",
    "    def get_property_type(row):\n",
    "        if pd.notna(row['property_type']):\n",
    "            return row['property_type']\n",
    "        title = str(row['title']).lower()\n",
    "        if 'house' in title: return 'House'\n",
    "        if 'flat' in title or 'apartment' in title: return 'Flat'\n",
    "        if 'land' in title or 'stand' in title: return 'Land'\n",
    "        if 'commercial' in title or 'office' in title: return 'Commercial'\n",
    "        return 'Other'\n",
    "    \n",
    "    df_clean['property_type_clean'] = df_clean.apply(get_property_type, axis=1)\n",
    "    \n",
    "    #Location encoding\n",
    "    df_clean['location'] = df_clean['location'].fillna('Unknown')\n",
    "    \n",
    "    #FInal features for dataframe\n",
    "    feature_columns = [\n",
    "        'building_area', 'land_area', 'bedrooms', 'bathrooms',\n",
    "        'has_building', 'has_land', 'area_ratio', 'bed_bath_ratio', \n",
    "        'price_per_sqm', 'image_count'\n",
    "    ]\n",
    "    \n",
    "    X = df_clean[feature_columns].copy()\n",
    "    y = df_clean['price']\n",
    "    \n",
    "    # Encoding\n",
    "    property_encoder = LabelEncoder()\n",
    "    location_encoder = LabelEncoder()\n",
    "    \n",
    "    X['property_type_encoded'] = property_encoder.fit_transform(df_clean['property_type_clean'])\n",
    "    X['location_encoded'] = location_encoder.fit_transform(df_clean['location'])\n",
    "    \n",
    "    return X, y, df_clean\n",
    "\n",
    "# Apply preprocessing\n",
    "X, y, df_clean = preprocess_property_data(df)\n",
    "\n",
    "print(f\"Final features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Features: {X.columns.tolist()}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861381aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42b91f9",
   "metadata": {},
   "source": [
    "**Image Processing Pipeline**\n",
    "\n",
    "This image processing pipeline implements an innovative two-pass *median imputation* strategy that fundamentally addresses the common challenge of missing image data in real-world datasets. The approach begins with a comprehensive first pass through all property listings, systematically collecting every available valid image while intelligently handling various edge cases including missing filenames, file system errors, and corrupted images. This careful collection phase enables the computation of a dataset-wide median image that statistically represents the central tendency of all available property photographs.\n",
    "\n",
    "The core innovation lies in the second pass, where every property listing receives either its actual image or the computed median as a sophisticated placeholder. This strategy ensures complete data preservation - no property is discarded due to missing images, *maintaining the statistical power of the full 1,613-sample dataset*. The median image serves as a *domain-aware default that provides more meaningful information than simple zero-padding*, as it contains realistic visual patterns and color distributions characteristic of property photographs in the Zimbabwe market.\n",
    "\n",
    "The implementation demonstrates robust error handling through multiple validation layers: filename sanity checks, filesystem existence verification, and loading exception management. Comprehensive status tracking provides transparency into the loading process, enabling quality assessment and potential troubleshooting. The final output maintains perfect alignment between image data and tabular features, ensuring that machine learning models can learn from both data modalities without introducing biases from missing visual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70aca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images (missing images will be replaced with median image)...\n",
      "Computed median image from 466 loaded images\n",
      "\n",
      "Image loading summary:\n",
      "  median_imputed: 1144 images\n",
      "  loaded: 466 images\n",
      "Total images processed: 1610\n",
      "Image array shape: (1610, 224, 224, 3)\n",
      "\n",
      "Final dataset sizes (ALL data preserved):\n",
      "Images: (1610, 224, 224, 3)\n",
      "Features (X): (1610, 12)\n",
      "Targets (y): (1610,)\n",
      "DataFrame: (1610, 21)\n",
      "\n",
      "Image Loading Details:\n",
      "Successfully loaded: 466\n",
      "Median imputed: 1144\n",
      "Load errors (median used): 0\n",
      "\n",
      "Median Image Statistics:\n",
      "Shape: (224, 224, 3)\n",
      "Min pixel value: 0.276\n",
      "Max pixel value: 0.827\n",
      "Mean pixel value: 0.495\n"
     ]
    }
   ],
   "source": [
    "def load_images_with_median_imputation(df_clean, image_folder=r\"C:\\Users\\Brendon\\Desktop\\Deep_Learning\\ASSIGNMENTS\\images\", target_size=(224, 224)):\n",
    "    \"\"\"Load images, replace missing ones with median of available images\"\"\"\n",
    "    \n",
    "    image_arrays = []\n",
    "    image_status = []\n",
    "    loaded_images = []  # Store successfully loaded images to compute median\n",
    "    \n",
    "    print(\"Loading images (missing images will be replaced with median image)...\")\n",
    "    \n",
    "    # First pass: collect all successfully loaded images\n",
    "    for idx, filename in enumerate(df_clean['image_filenames']):\n",
    "        if pd.isna(filename) or filename is None or filename == '':\n",
    "            continue\n",
    "            \n",
    "        filename_str = str(filename).strip()\n",
    "        if filename_str.lower() in ['nan', 'none', 'null', '']:\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(image_folder, filename_str)\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "                \n",
    "            img = load_img(img_path, target_size=target_size)\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = img_array / 255.0\n",
    "            loaded_images.append(img_array)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Compute median image from loaded images\n",
    "    if loaded_images:\n",
    "        median_image = np.median(loaded_images, axis=0)\n",
    "        print(f\"Computed median image from {len(loaded_images)} loaded images\")\n",
    "    else:\n",
    "        median_image = np.zeros((*target_size, 3))\n",
    "        print(\"No images loaded, using zero image as median\")\n",
    "    \n",
    "    # Second pass: build final array with median imputation\n",
    "    for idx, filename in enumerate(df_clean['image_filenames']):\n",
    "        # Start with median as default\n",
    "        current_array = median_image.copy()\n",
    "        current_status = 'median_imputed'\n",
    "        \n",
    "        # Check if filename is valid\n",
    "        if pd.isna(filename) or filename is None or filename == '':\n",
    "            image_arrays.append(current_array)\n",
    "            image_status.append(current_status)\n",
    "            continue\n",
    "            \n",
    "        filename_str = str(filename).strip()\n",
    "        if filename_str.lower() in ['nan', 'none', 'null', '']:\n",
    "            image_arrays.append(current_array)\n",
    "            image_status.append(current_status)\n",
    "            continue\n",
    "        \n",
    "        # Try to load actual image\n",
    "        img_path = os.path.join(image_folder, filename_str)\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                image_arrays.append(current_array)\n",
    "                image_status.append(current_status)\n",
    "                continue\n",
    "                \n",
    "            # Load and resize image\n",
    "            img = load_img(img_path, target_size=target_size)\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = img_array / 255.0\n",
    "            \n",
    "            image_arrays.append(img_array)\n",
    "            image_status.append('loaded')\n",
    "            \n",
    "        except Exception as e:\n",
    "            image_arrays.append(current_array)\n",
    "            image_status.append('load_error_median')\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    image_arrays = np.array(image_arrays)\n",
    "    \n",
    "    # Print statistics\n",
    "    status_counts = pd.Series(image_status).value_counts()\n",
    "    print(f\"\\nImage loading summary:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} images\")\n",
    "    print(f\"Total images processed: {len(image_arrays)}\")\n",
    "    print(f\"Image array shape: {image_arrays.shape}\")\n",
    "    \n",
    "    return image_arrays, image_status, median_image\n",
    "\n",
    "def create_complete_dataset_with_median(df_clean, X_scaled, y):\n",
    "    \"\"\"Create dataset with all tabular data, replacing missing images with median\"\"\"\n",
    "    \n",
    "    # Load images with median imputation\n",
    "    images, image_status, median_image = load_images_with_median_imputation(df_clean)\n",
    "    \n",
    "    # Use ALL tabular data\n",
    "    df_with_images = df_clean.copy()\n",
    "    X_with_images = X_scaled\n",
    "    y_with_images = y\n",
    "    \n",
    "    print(f\"\\nFinal dataset sizes (ALL data preserved):\")\n",
    "    print(f\"Images: {images.shape}\")\n",
    "    print(f\"Features (X): {X_with_images.shape}\")\n",
    "    print(f\"Targets (y): {y_with_images.shape}\")\n",
    "    print(f\"DataFrame: {df_with_images.shape}\")\n",
    "    \n",
    "    return images, X_with_images, y_with_images, df_with_images, image_status, median_image\n",
    "\n",
    "# Load with median imputation\n",
    "images, X_with_images, y_with_images, df_with_images, image_status, median_image = create_complete_dataset_with_median(df_clean, X_scaled, y)\n",
    "\n",
    "# Show detailed statistics\n",
    "status_series = pd.Series(image_status)\n",
    "print(f\"\\nImage Loading Details:\")\n",
    "print(f\"Successfully loaded: {sum(status_series == 'loaded')}\")\n",
    "print(f\"Median imputed: {sum(status_series == 'median_imputed')}\")\n",
    "print(f\"Load errors (median used): {sum(status_series == 'load_error_median')}\")\n",
    "\n",
    "# Optional: Display median image stats\n",
    "print(f\"\\nMedian Image Statistics:\")\n",
    "print(f\"Shape: {median_image.shape}\")\n",
    "print(f\"Min pixel value: {median_image.min():.3f}\")\n",
    "print(f\"Max pixel value: {median_image.max():.3f}\")\n",
    "print(f\"Mean pixel value: {median_image.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73a71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9192ad1",
   "metadata": {},
   "source": [
    "This comprehensive CNN architecture import strategy represents a meticulously curated selection of 12 convolutional neural networks that span the entire evolutionary spectrum of modern deep learning for computer vision. The implementation demonstrates sophisticated foresight in architectural diversity, ensuring that the comparative analysis covers all major design paradigms that have shaped the field. Each imported architecture brings distinct theoretical foundations and practical characteristics, from the groundbreaking residual connections of ResNet to the computationally efficient compound scaling of EfficientNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40ae258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import (\n",
    "    ResNet50, EfficientNetB0, DenseNet121, MobileNetV2, VGG16, Xception,\n",
    "    ResNet152, EfficientNetB3, DenseNet201, NASNetMobile, InceptionV3, InceptionResNetV2\n",
    ")\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as inception_resnet_preprocess\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input as nasnet_preprocess\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5837f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b09899",
   "metadata": {},
   "source": [
    "**CNN Feature Extraction Pipeline: Systematic Architecture Benchmarking**\n",
    "\n",
    "Analyzing property photos using 12 different AI models to find visual patterns that help predict house prices. Each model looks at the images in its own unique way, and see which approach works best for real estate.\n",
    "\n",
    "How It Works - Simple Explanation\n",
    "1. Two-Step Processing\n",
    " - First, prepare each image specifically for each model\n",
    " - Some models like smaller images (224x224 pixels)\n",
    " - Others need larger ones (299x299 or 300x300 pixels)\n",
    "\n",
    "2. Extract feature vectors\n",
    " - Each model converts images into arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24142b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brendon\\AppData\\Local\\Temp\\ipykernel_14820\\2054514194.py:63: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  MobileNetV2(weights='imagenet', include_top=False, pooling='avg'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Brendon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "=== CNN MODELS FOR FEATURE EXTRACTION ===\n",
      "\n",
      "Model details:\n",
      "ResNet50             | Input: (224, 224) | Output: 2048 features\n",
      "EfficientNetB0       | Input: (224, 224) | Output: 1280 features\n",
      "DenseNet121          | Input: (224, 224) | Output: 1024 features\n",
      "MobileNetV2          | Input: (224, 224) | Output: 1280 features\n",
      "VGG16                | Input: (224, 224) | Output: 512 features\n",
      "Xception             | Input: (299, 299) | Output: 2048 features\n",
      "ResNet152            | Input: (224, 224) | Output: 2048 features\n",
      "EfficientNetB3       | Input: (300, 300) | Output: 1536 features\n",
      "DenseNet201          | Input: (224, 224) | Output: 1920 features\n",
      "NASNetMobile         | Input: (224, 224) | Output: 1056 features\n",
      "InceptionV3          | Input: (299, 299) | Output: 2048 features\n",
      "InceptionResNetV2    | Input: (299, 299) | Output: 1536 features\n",
      "\n",
      "============================================================\n",
      "FEATURE EXTRACTION WITH 12 MODELS\n",
      "============================================================\n",
      "Extracting features with ResNet50...\n",
      "  ResNet50: (1610, 2048) - 1610/1610 successful\n",
      "Extracting features with EfficientNetB0...\n",
      "  EfficientNetB0: (1610, 1280) - 1610/1610 successful\n",
      "Extracting features with DenseNet121...\n",
      "  DenseNet121: (1610, 1024) - 1610/1610 successful\n",
      "Extracting features with MobileNetV2...\n",
      "  MobileNetV2: (1610, 1280) - 1610/1610 successful\n",
      "Extracting features with VGG16...\n",
      "  VGG16: (1610, 512) - 1610/1610 successful\n",
      "Extracting features with Xception...\n",
      "  Xception: (1610, 2048) - 1610/1610 successful\n",
      "Extracting features with ResNet152...\n",
      "  ResNet152: (1610, 2048) - 1610/1610 successful\n",
      "Extracting features with EfficientNetB3...\n",
      "  EfficientNetB3: (1610, 1536) - 1610/1610 successful\n",
      "Extracting features with DenseNet201...\n",
      "  DenseNet201: (1610, 1920) - 1610/1610 successful\n",
      "Extracting features with NASNetMobile...\n",
      "  NASNetMobile: (1610, 1056) - 1610/1610 successful\n",
      "Extracting features with InceptionV3...\n",
      "  InceptionV3: (1610, 2048) - 1610/1610 successful\n",
      "Extracting features with InceptionResNetV2...\n",
      "  InceptionResNetV2: (1610, 1536) - 1610/1610 successful\n",
      "\n",
      "============================================================\n",
      "FEATURE EXTRACTION COMPLETED\n",
      "============================================================\n",
      "ResNet50            : (1610, 2048)\n",
      "EfficientNetB0      : (1610, 1280)\n",
      "DenseNet121         : (1610, 1024)\n",
      "MobileNetV2         : (1610, 1280)\n",
      "VGG16               : (1610, 512)\n",
      "Xception            : (1610, 2048)\n",
      "ResNet152           : (1610, 2048)\n",
      "EfficientNetB3      : (1610, 1536)\n",
      "DenseNet201         : (1610, 1920)\n",
      "NASNetMobile        : (1610, 1056)\n",
      "InceptionV3         : (1610, 2048)\n",
      "InceptionResNetV2   : (1610, 1536)\n"
     ]
    }
   ],
   "source": [
    "def extract_cnn_features(image_arrays, models_config):\n",
    "\n",
    "    features_dict = {}\n",
    "    \n",
    "    for model_name, (model, preprocess_fn, target_size) in models_config.items():\n",
    "        print(f\"Extracting features with {model_name}...\")\n",
    "        \n",
    "        features = []\n",
    "        successful_extractions = 0\n",
    "        \n",
    "        for img_array in image_arrays:\n",
    "            try:\n",
    "                # Resizing for models that require specific input sizes\n",
    "                if target_size[0] != img_array.shape[0] or target_size[1] != img_array.shape[1]:\n",
    "                    from PIL import Image\n",
    "                    img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "                    img = img.resize(target_size)\n",
    "                    img_resized = np.array(img) / 255.0\n",
    "                else:\n",
    "                    img_resized = img_array\n",
    "                \n",
    "                # Preprocess for specific model\n",
    "                img_processed = preprocess_fn(np.expand_dims(img_resized * 255, axis=0))\n",
    "                \n",
    "                # Extract features\n",
    "                feature = model.predict(img_processed, verbose=0)\n",
    "                features.append(feature.flatten())\n",
    "                successful_extractions += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Use zeros with correct dimension as fallback\n",
    "                feature_shape = model.output_shape\n",
    "                if len(feature_shape) == 4:\n",
    "                    zero_features = np.zeros(np.prod(feature_shape[1:])).flatten()\n",
    "                else:\n",
    "                    zero_features = np.zeros(feature_shape[1]).flatten()\n",
    "                features.append(zero_features)\n",
    "                print(f\"  Warning: Feature extraction failed for one image, using zeros\")\n",
    "        \n",
    "        features_dict[model_name] = np.array(features)\n",
    "        print(f\"  {model_name}: {features_dict[model_name].shape} - {successful_extractions}/{len(image_arrays)} successful\")\n",
    "    \n",
    "    return features_dict\n",
    "\n",
    "# Define models configuration\n",
    "models_config = {\n",
    "    'ResNet50': (\n",
    "        ResNet50(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        resnet_preprocess, \n",
    "        (224, 224)\n",
    "    ),\n",
    "    'EfficientNetB0': (\n",
    "        EfficientNetB0(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        efficientnet_preprocess, \n",
    "        (224, 224)\n",
    "    ),\n",
    "    'DenseNet121': (\n",
    "        DenseNet121(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        densenet_preprocess, \n",
    "        (224, 224)\n",
    "    ),\n",
    "    'MobileNetV2': (\n",
    "        MobileNetV2(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        mobilenet_preprocess, \n",
    "        (224, 224)\n",
    "    ),\n",
    "    'VGG16': (\n",
    "        VGG16(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        vgg_preprocess, \n",
    "        (224, 224)\n",
    "    ),\n",
    "    'Xception': (\n",
    "        Xception(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        xception_preprocess, \n",
    "        (299, 299)\n",
    "    ),\n",
    "    \n",
    "    'ResNet152': (\n",
    "        ResNet152(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        resnet_preprocess, \n",
    "        (224, 224)\n",
    "    ),\n",
    "    'EfficientNetB3': (\n",
    "        EfficientNetB3(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        efficientnet_preprocess, \n",
    "        (300, 300)  # EfficientNetB3 prefers 300x300\n",
    "    ),\n",
    "    'DenseNet201': (\n",
    "        DenseNet201(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        densenet_preprocess, \n",
    "        (224, 224)\n",
    "    ),\n",
    "    'NASNetMobile': (\n",
    "        NASNetMobile(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        nasnet_preprocess, \n",
    "        (224, 224)\n",
    "    ),\n",
    "    'InceptionV3': (\n",
    "        InceptionV3(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        inception_preprocess, \n",
    "        (299, 299)  # InceptionV3 requires 299x299\n",
    "    ),\n",
    "    'InceptionResNetV2': (\n",
    "        InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg'), \n",
    "        inception_resnet_preprocess, \n",
    "        (299, 299)  # InceptionResNetV2 requires 299x299\n",
    "    )\n",
    "}\n",
    "\n",
    "# Print model information\n",
    "print(\"=== CNN MODELS FOR FEATURE EXTRACTION ===\")\n",
    "print(\"\\nModel details:\")\n",
    "\n",
    "for model_name, (model, _, target_size) in models_config.items():\n",
    "    feature_shape = model.output_shape\n",
    "    feature_dim = feature_shape[1] if len(feature_shape) == 2 else np.prod(feature_shape[1:])\n",
    "    print(f\"{model_name:20} | Input: {target_size} | Output: {feature_dim} features\")\n",
    "\n",
    "# EXTRACT FEATURES\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE EXTRACTION WITH 12 MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cnn_features = extract_cnn_features(images, models_config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE EXTRACTION COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, features in cnn_features.items():\n",
    "    print(f\"{model_name:20}: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a9fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e75480",
   "metadata": {},
   "source": [
    "**Dataset Integration: Multi-Modal Feature Fusion**  \n",
    "High-Level Overview  \n",
    "\n",
    "We're now performing feature fusion - combining our structured tabular data with the unstructured visual features extracted from our CNN architectures. This creates multi-modal datasets where each property is represented by both its metadata features and its learned visual representations.\n",
    "\n",
    "For each of our 12 CNN backbones, we're creating an integrated feature space that merges:  \n",
    "- Structured attributes: The original 12 engineered features (bedrooms, location encodings, area ratios, etc.)\n",
    "- Visual embeddings: The high-dimensional feature vectors from each architecture's convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc1f2243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING COMBINED DATASETS\n",
      "============================================================\n",
      "ResNet50: Tabular 12 + CNN 2048 = Combined 2060\n",
      "EfficientNetB0: Tabular 12 + CNN 1280 = Combined 1292\n",
      "DenseNet121: Tabular 12 + CNN 1024 = Combined 1036\n",
      "MobileNetV2: Tabular 12 + CNN 1280 = Combined 1292\n",
      "VGG16: Tabular 12 + CNN 512 = Combined 524\n",
      "Xception: Tabular 12 + CNN 2048 = Combined 2060\n",
      "ResNet152: Tabular 12 + CNN 2048 = Combined 2060\n",
      "EfficientNetB3: Tabular 12 + CNN 1536 = Combined 1548\n",
      "DenseNet201: Tabular 12 + CNN 1920 = Combined 1932\n",
      "NASNetMobile: Tabular 12 + CNN 1056 = Combined 1068\n",
      "InceptionV3: Tabular 12 + CNN 2048 = Combined 2060\n",
      "InceptionResNetV2: Tabular 12 + CNN 1536 = Combined 1548\n",
      "\n",
      "============================================================\n",
      "SAVING EXPANDED FEATURES\n",
      "============================================================\n",
      "All expanded features saved successfully!\n",
      "\n",
      "============================================================\n",
      "FINAL EXPANDED SUMMARY\n",
      "============================================================\n",
      "Images loaded: (1610, 224, 224, 3)\n",
      "Tabular features: (1610, 12)\n",
      "Target variable: (1610,)\n",
      "CNN features extracted for 12 models\n",
      "\n",
      "Model                | CNN Features | Total Features | Input Size\n",
      "-----------------------------------------------------------------\n",
      "ResNet50             |         2048 |           2060 | (224, 224)\n",
      "EfficientNetB0       |         1280 |           1292 | (224, 224)\n",
      "DenseNet121          |         1024 |           1036 | (224, 224)\n",
      "MobileNetV2          |         1280 |           1292 | (224, 224)\n",
      "VGG16                |          512 |            524 | (224, 224)\n",
      "Xception             |         2048 |           2060 | (299, 299)\n",
      "ResNet152            |         2048 |           2060 | (224, 224)\n",
      "EfficientNetB3       |         1536 |           1548 | (300, 300)\n",
      "DenseNet201          |         1920 |           1932 | (224, 224)\n",
      "NASNetMobile         |         1056 |           1068 | (224, 224)\n",
      "InceptionV3          |         2048 |           2060 | (299, 299)\n",
      "InceptionResNetV2    |         1536 |           1548 | (299, 299)\n"
     ]
    }
   ],
   "source": [
    "# Create combined datasets for each model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING COMBINED DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_combined_datasets(X_with_images, cnn_features, model_names=None):\n",
    "    \"\"\"\n",
    "    Create combined feature sets for each CNN model\n",
    "    \"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = list(cnn_features.keys())\n",
    "    \n",
    "    combined_datasets = {}\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Combine tabular features with CNN features\n",
    "        cnn_feature_array = cnn_features[model_name]\n",
    "        combined_features = np.concatenate([X_with_images, cnn_feature_array], axis=1)\n",
    "        combined_datasets[model_name] = combined_features\n",
    "        \n",
    "        print(f\"{model_name}: Tabular {X_with_images.shape[1]} + CNN {cnn_feature_array.shape[1]} = Combined {combined_features.shape[1]}\")\n",
    "    \n",
    "    return combined_datasets\n",
    "\n",
    "combined_datasets = create_combined_datasets(X_with_images, cnn_features)\n",
    "\n",
    "# Save expanded features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING EXPANDED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save CNN features\n",
    "with open('cnn_features_expanded.pkl', 'wb') as f:\n",
    "    pickle.dump(cnn_features, f)\n",
    "\n",
    "# Save combined datasets\n",
    "with open('combined_datasets_expanded.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_datasets, f)\n",
    "\n",
    "print(\"All expanded features saved successfully!\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXPANDED SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Images loaded: {images.shape}\")\n",
    "print(f\"Tabular features: {X_with_images.shape}\")\n",
    "print(f\"Target variable: {y_with_images.shape}\")\n",
    "print(f\"CNN features extracted for {len(cnn_features)} models\")\n",
    "\n",
    "# Display feature dimensions in a nice table\n",
    "print(f\"\\n{'Model':20} | {'CNN Features':12} | {'Total Features':14} | {'Input Size':10}\")\n",
    "print(\"-\" * 65)\n",
    "for model_name, features in cnn_features.items():\n",
    "    combined_shape = combined_datasets[model_name].shape\n",
    "    input_size = models_config[model_name][2]\n",
    "    print(f\"{model_name:20} | {features.shape[1]:12} | {combined_shape[1]:14} | {str(input_size):10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae139c9",
   "metadata": {},
   "source": [
    "**Multi-Modal Architecture Evaluation**\n",
    "\n",
    "We're now executing our core comparative analysis - training and evaluating neural networks on each of our 12 multi-modal datasets to determine which CNN backbone produces the most effective visual embeddings for property price prediction. This represents the model evaluation phase where we systematically measure how well each architecture's feature representations complement our tabular data.\n",
    "\n",
    "**Evaluation Strategy**\n",
    "- Consistent Splits: Same train/val/test indices across all architectures\n",
    "- Early Stopping: Prevents overfitting and ensures fair comparison\n",
    "- Multiple Metrics: MAE for monetary interpretation, RÂ² for variance explanation\n",
    "- Architecture Families: Enables pattern recognition across design paradigms\n",
    "\n",
    "**Comparative Performance Metrics**  \n",
    "\n",
    "The ranking table provides:  \n",
    "- Absolute Performance: Test MAE values in monetary terms\n",
    "- Explanatory Power: RÂ² scores indicating variance captured\n",
    "- Feature Efficiency: Relationship between embedding size and performance\n",
    "- Architecture Patterns: Performance trends across design families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38400ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparison of 12 CNN architectures...\n",
      "=== COMPARING 12 CNN ARCHITECTURES WITH NEURAL NETWORKS ===\n",
      "Training on 966 samples, Validating on 322 samples, Testing on 322 samples\n",
      "Tabular features: 12 dimensions\n",
      "\n",
      " 1/12 - Training with ResNet50            ... MAE: $631,169.31 | RÂ²: -0.064 | Features: 2048\n",
      " 2/12 - Training with EfficientNetB0      ... MAE: $631,203.56 | RÂ²: -0.064 | Features: 1280\n",
      " 3/12 - Training with DenseNet121         ... MAE: $631,194.44 | RÂ²: -0.064 | Features: 1024\n",
      " 4/12 - Training with MobileNetV2         ... MAE: $631,190.69 | RÂ²: -0.064 | Features: 1280\n",
      " 5/12 - Training with VGG16               ... MAE: $631,195.75 | RÂ²: -0.064 | Features: 512\n",
      " 6/12 - Training with Xception            ... MAE: $631,204.31 | RÂ²: -0.064 | Features: 2048\n",
      " 7/12 - Training with ResNet152           ... MAE: $631,206.19 | RÂ²: -0.064 | Features: 2048\n",
      " 8/12 - Training with EfficientNetB3      ... MAE: $631,202.31 | RÂ²: -0.064 | Features: 1536\n",
      " 9/12 - Training with DenseNet201         ... MAE: $631,166.94 | RÂ²: -0.064 | Features: 1920\n",
      "10/12 - Training with NASNetMobile        ... MAE: $631,204.56 | RÂ²: -0.064 | Features: 1056\n",
      "11/12 - Training with InceptionV3         ... MAE: $631,187.06 | RÂ²: -0.064 | Features: 2048\n",
      "12/12 - Training with InceptionResNetV2   ... MAE: $631,210.00 | RÂ²: -0.064 | Features: 1536\n",
      "\n",
      "======================================================================\n",
      "FINAL RANKINGS - 12 CNN ARCHITECTURES (Neural Networks)\n",
      "======================================================================\n",
      "Rank Model                Test MAE     RÂ²       CNN Features   Total Features\n",
      "----------------------------------------------------------------------\n",
      " 1.  DenseNet201          $631,166.94  -0.064          1920           1932\n",
      " 2.  ResNet50             $631,169.31  -0.064          2048           2060\n",
      " 3.  InceptionV3          $631,187.06  -0.064          2048           2060\n",
      " 4.  MobileNetV2          $631,190.69  -0.064          1280           1292\n",
      " 5.  DenseNet121          $631,194.44  -0.064          1024           1036\n",
      " 6.  VGG16                $631,195.75  -0.064           512            524\n",
      " 7.  EfficientNetB3       $631,202.31  -0.064          1536           1548\n",
      " 8.  EfficientNetB0       $631,203.56  -0.064          1280           1292\n",
      " 9.  Xception             $631,204.31  -0.064          2048           2060\n",
      "10.  NASNetMobile         $631,204.56  -0.064          1056           1068\n",
      "11.  ResNet152            $631,206.19  -0.064          2048           2060\n",
      "12.  InceptionResNetV2    $631,210.00  -0.064          1536           1548\n",
      "\n",
      "======================================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "======================================================================\n",
      "Best Model: DenseNet201 (MAE: $631,166.94)\n",
      "Worst Model: InceptionResNetV2 (MAE: $631,210.00)\n",
      "Average MAE across all models: $631,194.59 Â± $13.49\n",
      "Performance range: $631,166.94 - $631,210.00\n",
      "\n",
      "======================================================================\n",
      "ARCHITECTURE FAMILY ANALYSIS\n",
      "======================================================================\n",
      "ResNet          | Avg MAE: $631,187.75 | Best: ResNet50        ($631,169.31)\n",
      "EfficientNet    | Avg MAE: $631,202.94 | Best: EfficientNetB3  ($631,202.31)\n",
      "DenseNet        | Avg MAE: $631,180.69 | Best: DenseNet201     ($631,166.94)\n",
      "Mobile          | Avg MAE: $631,197.62 | Best: MobileNetV2     ($631,190.69)\n",
      "Inception       | Avg MAE: $631,198.53 | Best: InceptionV3     ($631,187.06)\n",
      "Classic         | Avg MAE: $631,200.03 | Best: VGG16           ($631,195.75)\n",
      "\n",
      "Results saved to 'nn_comparison_12_models.pkl'\n",
      "Comparison completed! ðŸŽ¯\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "def create_neural_network(tabular_dim, cnn_feature_dim):\n",
    "    \"\"\"Create neural network with combined tabular + CNN features\"\"\"\n",
    "    \n",
    "    # Tabular features input\n",
    "    tabular_input = Input(shape=(tabular_dim,), name='tabular_input')\n",
    "    x_tab = Dense(128, activation='relu')(tabular_input)\n",
    "    x_tab = BatchNormalization()(x_tab)\n",
    "    x_tab = Dropout(0.3)(x_tab)\n",
    "    x_tab = Dense(64, activation='relu')(x_tab)\n",
    "    x_tab = Dropout(0.2)(x_tab)\n",
    "    \n",
    "    # CNN features input  \n",
    "    cnn_input = Input(shape=(cnn_feature_dim,), name='cnn_input')\n",
    "    x_cnn = Dense(256, activation='relu')(cnn_input)\n",
    "    x_cnn = BatchNormalization()(x_cnn)\n",
    "    x_cnn = Dropout(0.3)(x_cnn)\n",
    "    x_cnn = Dense(128, activation='relu')(x_cnn)\n",
    "    x_cnn = Dropout(0.2)(x_cnn)\n",
    "    \n",
    "    # Concatenate both streams\n",
    "    concatenated = Concatenate()([x_tab, x_cnn])\n",
    "    \n",
    "    # Final layers\n",
    "    x = Dense(64, activation='relu')(concatenated)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    \n",
    "    # Output layer (regression - no activation)\n",
    "    output = Dense(1, activation='linear', name='price_output')(x)\n",
    "    \n",
    "    model = Model(inputs=[tabular_input, cnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='mse', \n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compare_12_cnn_architectures(combined_datasets, X_with_images, y_with_images):\n",
    "    \"\"\"Compare all 12 CNN architectures using neural networks\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    tabular_dim = X_with_images.shape[1]\n",
    "    \n",
    "    # Single split for all models (60-20-20)\n",
    "    indices = np.arange(len(y_with_images))\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        indices, y_with_images, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"=== COMPARING 12 CNN ARCHITECTURES WITH NEURAL NETWORKS ===\")\n",
    "    print(f\"Training on {len(X_train)} samples, Validating on {len(X_val)} samples, Testing on {len(X_test)} samples\")\n",
    "    print(f\"Tabular features: {tabular_dim} dimensions\\n\")\n",
    "    \n",
    "    for i, model_name in enumerate(combined_datasets.keys(), 1):\n",
    "        print(f\"{i:2d}/12 - Training with {model_name:20}...\", end=\" \")\n",
    "        \n",
    "        # Get features and split into components\n",
    "        combined_features = combined_datasets[model_name]\n",
    "        cnn_feature_dim = combined_features.shape[1] - tabular_dim\n",
    "        \n",
    "        # Split and separate features\n",
    "        X_train_combined = combined_features[X_train]\n",
    "        X_val_combined = combined_features[X_val]\n",
    "        X_test_combined = combined_features[X_test]\n",
    "        \n",
    "        X_train_tabular = X_train_combined[:, :tabular_dim]\n",
    "        X_train_cnn = X_train_combined[:, tabular_dim:]\n",
    "        X_val_tabular = X_val_combined[:, :tabular_dim] \n",
    "        X_val_cnn = X_val_combined[:, tabular_dim:]\n",
    "        X_test_tabular = X_test_combined[:, :tabular_dim]\n",
    "        X_test_cnn = X_test_combined[:, tabular_dim:]\n",
    "        \n",
    "        # Create and train neural network\n",
    "        nn_model = create_neural_network(tabular_dim, cnn_feature_dim)\n",
    "        \n",
    "        early_stop = EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "        \n",
    "        history = nn_model.fit(\n",
    "            [X_train_tabular, X_train_cnn], y_train,\n",
    "            validation_data=([X_val_tabular, X_val_cnn], y_val),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_mae = nn_model.evaluate([X_test_tabular, X_test_cnn], y_test, verbose=0)\n",
    "        y_pred = nn_model.predict([X_test_tabular, X_test_cnn], verbose=0).flatten()\n",
    "        test_r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'test_mae': test_mae,\n",
    "            'test_r2': test_r2,\n",
    "            'cnn_feature_dim': cnn_feature_dim,\n",
    "            'total_features': combined_features.shape[1],\n",
    "            'epochs_trained': len(history.history['loss'])\n",
    "        }\n",
    "        \n",
    "        print(f\"MAE: ${test_mae:,.2f} | RÂ²: {test_r2:.3f} | Features: {cnn_feature_dim}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare all 12 architectures\n",
    "print(\"Starting comparison of 12 CNN architectures...\")\n",
    "results_nn = compare_12_cnn_architectures(combined_datasets, X_with_images, y_with_images)\n",
    "\n",
    "# Display comprehensive rankings\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RANKINGS - 12 CNN ARCHITECTURES (Neural Networks)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ranked_results = sorted(results_nn.items(), key=lambda x: x[1]['test_mae'])\n",
    "\n",
    "print(f\"{'Rank':4} {'Model':20} {'Test MAE':12} {'RÂ²':8} {'CNN Features':14} {'Total Features':14}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (model_name, metrics) in enumerate(ranked_results, 1):\n",
    "    print(f\"{i:2d}.  {model_name:20} ${metrics['test_mae']:>10,.2f}  {metrics['test_r2']:>6.3f}  \"\n",
    "          f\"{metrics['cnn_feature_dim']:>12}  {metrics['total_features']:>13}\")\n",
    "\n",
    "# Performance analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_model = ranked_results[0][0]\n",
    "best_mae = ranked_results[0][1]['test_mae']\n",
    "worst_model = ranked_results[-1][0]\n",
    "worst_mae = ranked_results[-1][1]['test_mae']\n",
    "\n",
    "mae_values = [metrics['test_mae'] for metrics in results_nn.values()]\n",
    "mean_mae = np.mean(mae_values)\n",
    "std_mae = np.std(mae_values)\n",
    "\n",
    "print(f\"Best Model: {best_model} (MAE: ${best_mae:,.2f})\")\n",
    "print(f\"Worst Model: {worst_model} (MAE: ${worst_mae:,.2f})\")\n",
    "print(f\"Average MAE across all models: ${mean_mae:,.2f} Â± ${std_mae:,.2f}\")\n",
    "print(f\"Performance range: ${best_mae:,.2f} - ${worst_mae:,.2f}\")\n",
    "\n",
    "# Architecture family analysis\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ARCHITECTURE FAMILY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "families = {\n",
    "    'ResNet': ['ResNet50', 'ResNet152'],\n",
    "    'EfficientNet': ['EfficientNetB0', 'EfficientNetB3'],\n",
    "    'DenseNet': ['DenseNet121', 'DenseNet201'],\n",
    "    'Mobile': ['MobileNetV2', 'NASNetMobile'],\n",
    "    'Inception': ['InceptionV3', 'InceptionResNetV2'],\n",
    "    'Classic': ['VGG16', 'Xception']\n",
    "}\n",
    "\n",
    "for family, models in families.items():\n",
    "    family_maes = [results_nn[model]['test_mae'] for model in models if model in results_nn]\n",
    "    if family_maes:\n",
    "        avg_mae = np.mean(family_maes)\n",
    "        best_family_model = min([(model, results_nn[model]['test_mae']) for model in models if model in results_nn], \n",
    "                               key=lambda x: x[1])\n",
    "        print(f\"{family:15} | Avg MAE: ${avg_mae:>8,.2f} | Best: {best_family_model[0]:15} (${best_family_model[1]:,.2f})\")\n",
    "\n",
    "# Save results\n",
    "import pickle\n",
    "with open('nn_comparison_12_models.pkl', 'wb') as f:\n",
    "    pickle.dump(results_nn, f)\n",
    "\n",
    "print(f\"\\nResults saved to 'nn_comparison_12_models.pkl'\")\n",
    "print(\"Comparison completed! ðŸŽ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf9138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a494c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
